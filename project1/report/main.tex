\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
%\usepackage{mathtools}
\usepackage{bm}
\usepackage[margin=1in]{geometry}

\title{Project 1 in FYS-STK4155}
\author{Adrian Martinsen Kleven & Simon Elias Schrader }
\date{Autumn 2020}
\usepackage{hyperref}

\usepackage{natbib}
\usepackage{graphicx}
\begin{document}

\maketitle
\tableofcontents

\listoffigures
\listoftables


\clearpage
\section{Abstract}
In this project, we use some good methods and some shitty methods to test how well a polynomial can interpolate a superfancy exponential function (protip: It can't) . Then we use the same approaches to see how well random geographic data can be approximated. We expect this to look horrible. We're sorry for abusing math in that way. We shall contemplate Seppuku.

\begin{figure}[H]
\centering
\includegraphics[scale=1.7]{universe}
\caption{Some random picture.}
\label{fig:universe}
\end{figure}
\section{Introduction}
\section{Methods}
\subsection{Linear Regression}
The aim of Linear Regression is to find the conditional distribution $p(y|\bm{x})$ for y given $\bm{x}$. In Linear Regression, a linear functional dependence is assumed, and the aim is hence to create a linear function that fits a set of $p$ predictor variables $\bm{x}$ to a target variable $\bm{y}$, where there are $n$ discrete observations for each of the $p$ predictor variables and the target variable. That is, the aim is to find a functional relationship of the form $f(\bm{x})=y$. \\
We assume hence that each target variable $y_i$ has a functional dependence of the predictor variables:
\begin{equation*}
y_i= \tilde y_i+\epsilon_i=\beta_0 + \beta_1x_{i0} + \beta_2x_{i1}+...+\beta_px_{i(p-1)} + \epsilon_i
\end{equation*}
As this is true for all y, this can be written in matrix form:
\begin{equation*}
\bm{y}= \bm{X\beta} + \bm{\epsilon}
\end{equation*}
where $\bm{y}$, and $\bm{\epsilon}$ are vectors of length n,  $\bm{\beta}$ is of length p and $\bm{X}$ is a matrix of size $n*p$. X is called the design matrix.
 We now want to find fitting parameters $\bm{\beta}$ that represent the best linear fit of y to the input data $\bm{x}$. Hence, we want the fit
 \begin{equation*}
 \bm{\tildey}= \bm{X\beta}
 \end{equation*}
 to be as close to the target data $\bm{y}$ as possible.
  There are several approaches to define the best linear fit, and in this article, we looked at three different methods that define the best fit in a different way each. All methods have in common that they define a loss function, which describes the deviation from the fit to the real data, that needs to be minimized.
\subsubsection{Ordinary Least Squares Regression}
In Ordinary Least Square regression, the loss function is defined as the following:
\begin{equation*}
C(\bm{\beta})= \frac{1}{n}\sum_{i=0}^n\left( y_i-\tilde y_i \right)^2
=\frac{1}{n}(\bm{y}-\bm{\tilde y})(\bm{y}-\bm{\tilde y})^T=
\frac{1}{n}(\bm{y}-\bm{X\beta})(\bm{y}-\bm{X\beta})^T
\end{equation*}
The solution minimizing this problem is found by deriving $C(\bm{\beta})$ with regards to each $\beta_i$ and setting each derivative equal to zero. Doing this, we end up with the following equation (in matrix notation)
\begin{equation*}
\frac{\partial C(\bm{\beta})}{\partial\bm{\beta}}=0=\bm{X^T}(\bm{y}-\bm{X\beta})
\end{equation*}
which gives us
\begin{equation*}
\bm{\beta}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
\end{equation*}
which can be solved when $(\bm{X}^T\bm{X})$ is invertible.

Under the assumption that true data y is given as
\begin{equation*}
\bm{y}=f(\bm{x})+\bm{\epsilon}
\end{equation*}
where $\bm{\epsilon}$ follows a normal distribution $N(0,\sigma^2)$, information about the variance and the expectation value can be deduced. Assuming that $f(\bm{x})=\bm{X\beta}$, it can be shown that
\begin{equation*}
\mathbb{E}(y_i)=\bm{X_{i,*}\beta}
\end{equation*}
\begin{equation*}
Var(y_i)=\sigma^2
\end{equation*}

where $\bm{X_{i,*}}$ is the i-th row of the design matrix.

For $\beta$, it can be shown that
\begin{equation*}
\mathbb{E}(\bm{\beta})=\bm{\beta}
\end{equation*}
\begin{equation*}
Var(\bm{\beta})=\sigma^2(\bm{X}^T\bm{X}})^{-1}
\end{equation*}
\subsubsection{Ridge Regression}
\subsubsection{LASSO Regression}
\subsection{Bias-variance trade-off}
\subsection{Resampling Methods and Data Splitting}
Here one might mention that the data has to be split in a test and a training set and why that should be done.
\subsubsection{Bootstrap method}
The Bootstrap method is a method that can be used to calculate statistical quantities. The main idea is the following: Let n be the size of the training set $Z=(z_1,z_2,...z_n)$, where each $z_i$ contains p predictors and one target value, that is $z_i=(x_{i,1},x_{i,2},...x_{i,p},y_i)$. Now, n data points $z_i$ are drawn randomly from Z with replacement. This is done $B$ times, creating $B$ new sets $Z'_j$. We can now create a fitting model for each of the sets  $Z'_j$. For each of these data sets $Z'_j$, we can now calculate any variable $\hat\theta_j$ given the data. Now, the distribution of $\hat\theta_j$ can be used to estimate the distribution of $\theta$ and get values of interest, such as the mean value or the variance. In this article, Bootstrap is used to get information about the test MSE, variance and bias in order to evaluate the quality of the fit. This is done by using the B bootstrap sets $Z'_j$ to calculate the fitting parameters $\beta$ and calculate B fits to the test set. $\tilde{Y_j}$. From these B fits, we can then approximate the test MSE, variance and bias.\\
TODO: Talk some more about the mathematical validity of Bootstrap.
\subsubsection{K-fold Cross validation}
\subsection{Fitting the Franke function}
\subsection{Oslo data?}
\section{Computational Implementation ? Possibly put this in methods?}
\subsection{Sampling Franke Function}
The Franke Function is given by
\begin{align*}
f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}
and serves as a test function to check how well the implemented methods beform.
We sample randomly points $x_i$ and $y_i$ between 0 and 1 and calculate the target points
\begin{equation*}
z_i=f(x_i,y_i)+\epsilon_i
\end{equation*}
where $\epsilon_i$ follow a normal distribution with standard deviation $\sigma$.
\subsection{The Design Matrix}
The Design Matrix implemented is a 2D-version of the Vandermonde Matrix, hence, approximate $z_i$ as, for a given degree $k$
\begin{equation*}
\tilde z_i=\beta_0+\beta_1x_i+\beta_2y_i+\beta_3x_i^2++\beta_2x_iy_i+...+\beta_py_i^k
\end{equation*}
In the practical implementation, we "drop" $\beta_0$ by removing the first column from the Design Matrix, as our data is scaled.
\subsection{Scaling?}
The design matrix is scaled using Scikit-Learn's StandardScaler.
\section{Results}
\section{Conclusion}

\section{Conclusion}
``I always thought something was fundamentally wrong with the universe'' \citep{hastie01statisticallearning}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
